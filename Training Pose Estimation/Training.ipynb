{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.196  Python-3.12.3 torch-2.3.0+cu118 CUDA:0 (NVIDIA GeForce RTX 2060 with Max-Q Design, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=pose, mode=train, model=yolov8n-pose.pt, data=coco-pose.yaml, epochs=20, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=Adam, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\pose\\train5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1035934  ultralytics.nn.modules.head.Pose             [1, [17, 3], [64, 128, 256]]  \n",
      "YOLOv8n-pose summary: 250 layers, 3295470 parameters, 3295454 gradients, 9.3 GFLOPs\n",
      "\n",
      "Transferred 397/397 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\pose\\train5', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "c:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\ARTIFICIAL INTELLIGENCE\\SEMESTER 1\\Computer Vision and Deep Learning\\CV_PROJECT_LAZY_TRAIN\\human-pose-estimation-opencv\\Chair_occupancy\\Training_Pose_Estimation\\dataset\\labels\\train.cache... 10000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 10000/10000 [00:00<?, ?it/s]\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.10 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n",
      "c:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\composition.py:144: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\ARTIFICIAL INTELLIGENCE\\SEMESTER 1\\Computer Vision and Deep Learning\\CV_PROJECT_LAZY_TRAIN\\human-pose-estimation-opencv\\Chair_occupancy\\Training_Pose_Estimation\\dataset\\labels\\val.cache... 2693 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2693/2693 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\pose\\train5\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001, momentum=0.937) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0001), 72 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns\\pose\\train5\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/20     0.742G      1.409      3.631     0.3778      1.322       1.35         58        640: 100%|██████████| 2500/2500 [05:01<00:00,  8.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95):   0%|          | 0/337 [00:00<?, ?it/s]c:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:40<00:00,  8.34it/s]\n",
      "                   all       2693      11004      0.716      0.513      0.591       0.36      0.635      0.328      0.319      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/20     0.843G      1.448      3.767     0.3833      1.371       1.38         43        640: 100%|██████████| 2500/2500 [05:20<00:00,  7.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:40<00:00,  8.33it/s]\n",
      "                   all       2693      11004      0.707      0.515       0.59      0.349      0.596       0.32      0.279      0.107\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/20     0.879G      1.451      3.778     0.3809      1.381      1.394         16        640: 100%|██████████| 2500/2500 [05:18<00:00,  7.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:38<00:00,  8.80it/s]\n",
      "                   all       2693      11004      0.713      0.516        0.6      0.361      0.615      0.311      0.295      0.112\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/20     0.875G       1.43      3.729     0.3769      1.355      1.377         16        640: 100%|██████████| 2500/2500 [05:08<00:00,  8.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.49it/s]\n",
      "                   all       2693      11004      0.712      0.542      0.619      0.379      0.626      0.326      0.314      0.126\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/20     0.877G      1.419      3.665     0.3697      1.337       1.37         14        640: 100%|██████████| 2500/2500 [05:07<00:00,  8.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.52it/s]\n",
      "                   all       2693      11004      0.714      0.536      0.617      0.374      0.622      0.326      0.306      0.128\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/20     0.864G      1.381      3.549     0.3679      1.292      1.341         24        640: 100%|██████████| 2500/2500 [05:07<00:00,  8.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.63it/s]\n",
      "                   all       2693      11004      0.726      0.546      0.632      0.395      0.635      0.343      0.328      0.142\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/20     0.872G      1.356      3.493     0.3618      1.266      1.334         35        640: 100%|██████████| 2500/2500 [05:08<00:00,  8.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:38<00:00,  8.67it/s]\n",
      "                   all       2693      11004      0.737       0.55      0.641      0.403      0.643      0.352      0.342       0.15\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/20     0.887G      1.347      3.417     0.3555      1.243      1.323         22        640: 100%|██████████| 2500/2500 [05:09<00:00,  8.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.61it/s]\n",
      "                   all       2693      11004      0.723      0.562      0.641      0.402      0.631      0.343      0.332       0.15\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/20     0.856G      1.327      3.319     0.3497      1.222      1.314         29        640: 100%|██████████| 2500/2500 [05:07<00:00,  8.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.60it/s]\n",
      "                   all       2693      11004      0.744      0.561       0.65      0.411      0.646      0.351      0.349      0.165\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/20     0.841G      1.312      3.296     0.3439      1.204        1.3         16        640: 100%|██████████| 2500/2500 [05:11<00:00,  8.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.52it/s]\n",
      "                   all       2693      11004      0.741      0.572      0.659      0.421      0.655      0.363      0.358      0.168\n",
      "Closing dataloader mosaic\n",
      "c:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\composition.py:144: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/20     0.849G      1.295      2.769      0.328      1.143      1.293         17        640: 100%|██████████| 2500/2500 [04:51<00:00,  8.57it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:38<00:00,  8.66it/s]\n",
      "                   all       2693      11004      0.751      0.563      0.653      0.419      0.648       0.36      0.356      0.171\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/20     0.828G      1.278      2.682     0.3202      1.113       1.28          9        640: 100%|██████████| 2500/2500 [05:07<00:00,  8.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:40<00:00,  8.34it/s]\n",
      "                   all       2693      11004      0.755      0.565      0.661      0.422       0.66      0.363      0.358      0.177\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/20      0.83G      1.251      2.616     0.3148      1.075      1.262         17        640: 100%|██████████| 2500/2500 [05:04<00:00,  8.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.60it/s]\n",
      "                   all       2693      11004      0.742      0.587      0.669      0.429      0.678      0.364      0.366      0.181\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/20     0.841G      1.239      2.574     0.3106      1.063      1.255          5        640: 100%|██████████| 2500/2500 [05:05<00:00,  8.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.52it/s]\n",
      "                   all       2693      11004      0.767      0.567      0.669      0.433      0.663      0.375      0.373      0.187\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/20     0.835G      1.227       2.51     0.3057       1.04      1.241         22        640: 100%|██████████| 2500/2500 [05:10<00:00,  8.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.56it/s]\n",
      "                   all       2693      11004      0.769      0.576      0.674      0.435      0.651      0.371      0.364      0.185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/20      0.83G      1.205      2.422        0.3       1.01      1.227          4        640: 100%|██████████| 2500/2500 [05:11<00:00,  8.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:41<00:00,  8.22it/s]\n",
      "                   all       2693      11004       0.75      0.586      0.677      0.442      0.701      0.378      0.384        0.2\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/20     0.828G      1.194      2.399     0.2963      1.001       1.22         31        640: 100%|██████████| 2500/2500 [05:14<00:00,  7.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:40<00:00,  8.41it/s]\n",
      "                   all       2693      11004      0.752      0.597      0.682      0.448      0.692      0.378      0.386      0.204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/20     0.822G      1.174      2.326     0.2923     0.9676      1.206         21        640: 100%|██████████| 2500/2500 [05:08<00:00,  8.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.56it/s]\n",
      "                   all       2693      11004      0.765      0.599       0.69      0.456      0.689      0.383       0.39      0.208\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/20      0.82G      1.156      2.279     0.2888     0.9578      1.197          8        640: 100%|██████████| 2500/2500 [05:11<00:00,  8.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.49it/s]\n",
      "                   all       2693      11004      0.764      0.605      0.693      0.457       0.68       0.38      0.388      0.209\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/20     0.824G      1.139      2.217     0.2886     0.9382      1.185         24        640: 100%|██████████| 2500/2500 [05:07<00:00,  8.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:39<00:00,  8.49it/s]\n",
      "                   all       2693      11004      0.762      0.606      0.694       0.46      0.682      0.392      0.391      0.212\n",
      "\n",
      "20 epochs completed in 1.954 hours.\n",
      "Optimizer stripped from runs\\pose\\train5\\weights\\last.pt, 6.8MB\n",
      "Optimizer stripped from runs\\pose\\train5\\weights\\best.pt, 6.8MB\n",
      "\n",
      "Validating runs\\pose\\train5\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.196  Python-3.12.3 torch-2.3.0+cu118 CUDA:0 (NVIDIA GeForce RTX 2060 with Max-Q Design, 6144MiB)\n",
      "c:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "YOLOv8n-pose summary (fused): 187 layers, 3289964 parameters, 0 gradients, 9.2 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 337/337 [00:36<00:00,  9.18it/s]\n",
      "                   all       2693      11004      0.762      0.606      0.694       0.46      0.681      0.392       0.39      0.212\n",
      "Speed: 0.3ms preprocess, 3.3ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\pose\\train5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.PoseMetrics object with attributes:\n",
       "\n",
       "ap_class_index: array([0])\n",
       "box: ultralytics.utils.metrics.Metric object\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x00000282A858CEC0>\n",
       "fitness: 0.7133542725136549\n",
       "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'metrics/precision(P)', 'metrics/recall(P)', 'metrics/mAP50(P)', 'metrics/mAP50-95(P)']\n",
       "maps: array([    0.67211])\n",
       "names: {0: 'person'}\n",
       "plot: True\n",
       "pose: ultralytics.utils.metrics.Metric object\n",
       "results_dict: {'metrics/precision(B)': 0.7616080775434088, 'metrics/recall(B)': 0.6061432206470374, 'metrics/mAP50(B)': 0.6942607471413937, 'metrics/mAP50-95(B)': 0.46029433867100183, 'metrics/precision(P)': 0.6805205100775209, 'metrics/recall(P)': 0.39185750636132316, 'metrics/mAP50(P)': 0.3902643444496344, 'metrics/mAP50-95(P)': 0.21181873172294488, 'fitness': 0.7133542725136549}\n",
       "save_dir: WindowsPath('runs/pose/train5')\n",
       "seg: ultralytics.utils.metrics.Metric object\n",
       "speed: {'preprocess': 0.3120234319812252, 'inference': 3.2659529577255957, 'loss': 0.00045877648603947157, 'postprocess': 1.336655473337269}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n-pose.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"coco-pose.yaml\", epochs=10, imgsz=640)\n",
    "\n",
    "'''\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def train_yolov8(model_path, data_path, epochs=10, imgsz=640, batch_size=4, learning_rate=0.01, optimizer='SGD', weight_decay=0.0005):\n",
    "    # Load a model\n",
    "    model = YOLO(model_path)  # load a pretrained model (recommended for training)\n",
    "\n",
    "    # Train the model with specified hyperparameters\n",
    "    results = model.train(\n",
    "        data=data_path,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch_size,\n",
    "        lr0=learning_rate,\n",
    "        optimizer=optimizer,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "model_path = \"yolov8n-pose.pt\"\n",
    "data_path = \"coco-pose.yaml\"\n",
    "epochs = 20\n",
    "imgsz = 640\n",
    "batch_size = 4  # Adjust based on your GPU memory\n",
    "learning_rate = 0.001\n",
    "optimizer = 'Adam'  # 'SGD' or 'Adam'\n",
    "weight_decay = 0.0001\n",
    "\n",
    "train_yolov8(model_path, data_path, epochs, imgsz, batch_size, learning_rate, optimizer, weight_decay)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
