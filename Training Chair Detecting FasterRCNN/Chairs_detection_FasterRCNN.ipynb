{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: Detected 0 instances\n",
      "Frame 1: Detected 0 instances\n",
      "Frame 2: Detected 0 instances\n",
      "Frame 3: Detected 0 instances\n",
      "Frame 4: Detected 0 instances\n",
      "Frame 5: Detected 0 instances\n",
      "Frame 6: Detected 0 instances\n",
      "Frame 7: Detected 0 instances\n",
      "Frame 8: Detected 0 instances\n",
      "Frame 9: Detected 0 instances\n",
      "Frame 10: Detected 0 instances\n",
      "Frame 11: Detected 0 instances\n",
      "Frame 12: Detected 0 instances\n",
      "Frame 13: Detected 0 instances\n",
      "Frame 14: Detected 0 instances\n",
      "Frame 15: Detected 0 instances\n",
      "Frame 16: Detected 0 instances\n",
      "Frame 17: Detected 0 instances\n",
      "Frame 18: Detected 0 instances\n",
      "Frame 19: Detected 0 instances\n",
      "Frame 20: Detected 0 instances\n",
      "Frame 21: Detected 0 instances\n",
      "Frame 22: Detected 0 instances\n",
      "Frame 23: Detected 0 instances\n",
      "Frame 24: Detected 0 instances\n",
      "Frame 25: Detected 0 instances\n",
      "Frame 26: Detected 0 instances\n",
      "Frame 27: Detected 0 instances\n",
      "Frame 28: Detected 0 instances\n",
      "Frame 29: Detected 0 instances\n",
      "Frame 30: Detected 0 instances\n",
      "Frame 31: Detected 0 instances\n",
      "Frame 32: Detected 0 instances\n",
      "Frame 33: Detected 0 instances\n",
      "Frame 34: Detected 0 instances\n",
      "Frame 35: Detected 0 instances\n",
      "Frame 36: Detected 0 instances\n",
      "Frame 37: Detected 0 instances\n",
      "Frame 38: Detected 0 instances\n",
      "Frame 39: Detected 0 instances\n",
      "Frame 40: Detected 0 instances\n",
      "Frame 41: Detected 0 instances\n",
      "Frame 42: Detected 0 instances\n",
      "Frame 43: Detected 0 instances\n",
      "Frame 44: Detected 0 instances\n",
      "Frame 45: Detected 0 instances\n",
      "Frame 46: Detected 0 instances\n",
      "Frame 47: Detected 0 instances\n",
      "Frame 48: Detected 0 instances\n",
      "Frame 49: Detected 0 instances\n",
      "Frame 50: Detected 0 instances\n",
      "Frame 51: Detected 0 instances\n",
      "Frame 52: Detected 0 instances\n",
      "Frame 53: Detected 0 instances\n",
      "Frame 54: Detected 0 instances\n",
      "Frame 55: Detected 0 instances\n",
      "Frame 56: Detected 0 instances\n",
      "Frame 57: Detected 0 instances\n",
      "Frame 58: Detected 0 instances\n",
      "Frame 59: Detected 0 instances\n",
      "Frame 60: Detected 0 instances\n",
      "Frame 61: Detected 0 instances\n",
      "Frame 62: Detected 0 instances\n",
      "Frame 63: Detected 0 instances\n",
      "Frame 64: Detected 0 instances\n",
      "Frame 65: Detected 0 instances\n",
      "Frame 66: Detected 0 instances\n",
      "Frame 67: Detected 0 instances\n",
      "Frame 68: Detected 0 instances\n",
      "Frame 69: Detected 0 instances\n",
      "Frame 70: Detected 0 instances\n",
      "Frame 71: Detected 0 instances\n",
      "Frame 72: Detected 0 instances\n",
      "Frame 73: Detected 0 instances\n",
      "Frame 74: Detected 0 instances\n",
      "Frame 75: Detected 0 instances\n",
      "Frame 76: Detected 0 instances\n",
      "Frame 77: Detected 0 instances\n",
      "Frame 78: Detected 0 instances\n",
      "Frame 79: Detected 0 instances\n",
      "Frame 80: Detected 0 instances\n",
      "Frame 81: Detected 0 instances\n",
      "Frame 82: Detected 0 instances\n",
      "Frame 83: Detected 0 instances\n",
      "Frame 84: Detected 0 instances\n",
      "Frame 85: Detected 0 instances\n",
      "Frame 86: Detected 0 instances\n",
      "Frame 87: Detected 0 instances\n",
      "Frame 88: Detected 0 instances\n",
      "Frame 89: Detected 0 instances\n",
      "Frame 90: Detected 0 instances\n",
      "Frame 91: Detected 0 instances\n",
      "Frame 92: Detected 0 instances\n",
      "Frame 93: Detected 0 instances\n",
      "Frame 94: Detected 0 instances\n",
      "Frame 95: Detected 0 instances\n",
      "Frame 96: Detected 0 instances\n",
      "Frame 97: Detected 0 instances\n",
      "Frame 98: Detected 0 instances\n",
      "Frame 99: Detected 0 instances\n",
      "Frame 100: Detected 0 instances\n",
      "Frame 101: Detected 0 instances\n",
      "Frame 102: Detected 0 instances\n",
      "Frame 103: Detected 0 instances\n",
      "Frame 104: Detected 0 instances\n",
      "Frame 105: Detected 0 instances\n",
      "Frame 106: Detected 0 instances\n",
      "Frame 107: Detected 0 instances\n",
      "Frame 108: Detected 0 instances\n",
      "Frame 109: Detected 0 instances\n",
      "Frame 110: Detected 0 instances\n",
      "Frame 111: Detected 0 instances\n",
      "Frame 112: Detected 0 instances\n",
      "Frame 113: Detected 0 instances\n",
      "Frame 114: Detected 0 instances\n",
      "Frame 115: Detected 0 instances\n",
      "Frame 116: Detected 0 instances\n",
      "Frame 117: Detected 0 instances\n",
      "Frame 118: Detected 0 instances\n",
      "Frame 119: Detected 0 instances\n",
      "Frame 120: Detected 0 instances\n",
      "Frame 121: Detected 0 instances\n",
      "Frame 122: Detected 0 instances\n",
      "Frame 123: Detected 0 instances\n",
      "Frame 124: Detected 0 instances\n",
      "Frame 125: Detected 0 instances\n",
      "Frame 126: Detected 0 instances\n",
      "Frame 127: Detected 0 instances\n",
      "Frame 128: Detected 0 instances\n",
      "Frame 129: Detected 0 instances\n",
      "Frame 130: Detected 0 instances\n",
      "Frame 131: Detected 0 instances\n",
      "Frame 132: Detected 0 instances\n",
      "Frame 133: Detected 0 instances\n",
      "Frame 134: Detected 0 instances\n",
      "Frame 135: Detected 0 instances\n",
      "Frame 136: Detected 0 instances\n",
      "Frame 137: Detected 0 instances\n",
      "Frame 138: Detected 0 instances\n",
      "Frame 139: Detected 0 instances\n",
      "Frame 140: Detected 0 instances\n",
      "Frame 141: Detected 0 instances\n",
      "Frame 142: Detected 0 instances\n",
      "Frame 143: Detected 0 instances\n",
      "Frame 144: Detected 0 instances\n",
      "Frame 145: Detected 0 instances\n",
      "Frame 146: Detected 0 instances\n",
      "Frame 147: Detected 0 instances\n",
      "Frame 148: Detected 0 instances\n",
      "Frame 149: Detected 0 instances\n",
      "Frame 150: Detected 0 instances\n",
      "Frame 151: Detected 0 instances\n",
      "Frame 152: Detected 0 instances\n",
      "Frame 153: Detected 0 instances\n",
      "Frame 154: Detected 0 instances\n",
      "Frame 155: Detected 0 instances\n",
      "Frame 156: Detected 0 instances\n",
      "Frame 157: Detected 0 instances\n",
      "Frame 158: Detected 0 instances\n",
      "Frame 159: Detected 0 instances\n",
      "Frame 160: Detected 0 instances\n",
      "Frame 161: Detected 0 instances\n",
      "Frame 162: Detected 0 instances\n",
      "Frame 163: Detected 0 instances\n",
      "Frame 164: Detected 0 instances\n",
      "Frame 165: Detected 0 instances\n",
      "Frame 166: Detected 0 instances\n",
      "Frame 167: Detected 0 instances\n",
      "Frame 168: Detected 0 instances\n",
      "Frame 169: Detected 0 instances\n",
      "Frame 170: Detected 0 instances\n",
      "Frame 171: Detected 0 instances\n",
      "Frame 172: Detected 0 instances\n",
      "Frame 173: Detected 0 instances\n",
      "Frame 174: Detected 0 instances\n",
      "Frame 175: Detected 0 instances\n",
      "Frame 176: Detected 0 instances\n",
      "Frame 177: Detected 0 instances\n",
      "Frame 178: Detected 0 instances\n",
      "Frame 179: Detected 0 instances\n",
      "Frame 180: Detected 0 instances\n",
      "Frame 181: Detected 0 instances\n",
      "Frame 182: Detected 0 instances\n",
      "Frame 183: Detected 0 instances\n",
      "Frame 184: Detected 0 instances\n",
      "Frame 185: Detected 0 instances\n",
      "Frame 186: Detected 0 instances\n",
      "Frame 187: Detected 0 instances\n",
      "Frame 188: Detected 0 instances\n",
      "Frame 189: Detected 0 instances\n",
      "Frame 190: Detected 0 instances\n",
      "Frame 191: Detected 0 instances\n",
      "Frame 192: Detected 0 instances\n",
      "Frame 193: Detected 0 instances\n",
      "Frame 194: Detected 0 instances\n",
      "Frame 195: Detected 0 instances\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m min_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m800\u001b[39m\n\u001b[0;32m    110\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/ARTIFICIAL INTELLIGENCE/SEMESTER 1/Computer Vision and Deep Learning/CV_PROJECT_LAZY_TRAIN/human-pose-estimation-opencv/Chair_occupancy/Training_chair_detection_FasterRCNN/video_output/output3_video_with_custom_model.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 112\u001b[0m \u001b[43mprocess_video_with_custom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m, in \u001b[0;36mprocess_video_with_custom_model\u001b[1;34m(input_path, model_path, min_size, output_path)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m instances \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m boxes \u001b[38;5;241m=\u001b[39m instances\u001b[38;5;241m.\u001b[39mpred_boxes\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\engine\\defaults.py:319\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[1;34m(self, original_image)\u001b[0m\n\u001b[0;32m    315\u001b[0m image\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[0;32m    317\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: image, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m: height, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: width}\n\u001b[1;32m--> 319\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\meta_arch\\rcnn.py:150\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, batched_inputs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_image(batched_inputs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\meta_arch\\rcnn.py:208\u001b[0m, in \u001b[0;36mGeneralizedRCNN.inference\u001b[1;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_instances \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m         proposals, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproposal_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproposals\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\proposal_generator\\rpn.py:452\u001b[0m, in \u001b[0;36mRPN.forward\u001b[1;34m(self, images, features, gt_instances)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    images (ImageList): input images of length `N`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m    loss: dict[Tensor] or None\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    451\u001b[0m features \u001b[38;5;241m=\u001b[39m [features[f] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features]\n\u001b[1;32m--> 452\u001b[0m anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manchor_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    454\u001b[0m pred_objectness_logits, pred_anchor_deltas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn_head(features)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# Transpose the Hi*Wi*A dimension to the middle:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\anchor_generator.py:230\u001b[0m, in \u001b[0;36mDefaultAnchorGenerator.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    features (list[Tensor]): list of backbone feature maps on which to generate anchors.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        where Hi, Wi are resolution of the feature map divided by anchor stride.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m grid_sizes \u001b[38;5;241m=\u001b[39m [feature_map\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m feature_map \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 230\u001b[0m anchors_over_all_feature_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grid_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_sizes\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pyre-ignore\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Boxes(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m anchors_over_all_feature_maps]\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\anchor_generator.py:174\u001b[0m, in \u001b[0;36mDefaultAnchorGenerator._grid_anchors\u001b[1;34m(self, grid_sizes)\u001b[0m\n\u001b[0;32m    172\u001b[0m buffers: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_anchors\u001b[38;5;241m.\u001b[39mnamed_buffers()]\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, stride, base_anchors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grid_sizes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides, buffers):\n\u001b[1;32m--> 174\u001b[0m     shift_x, shift_y \u001b[38;5;241m=\u001b[39m \u001b[43m_create_grid_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_anchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     shifts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack((shift_x, shift_y, shift_x, shift_y), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    177\u001b[0m     anchors\u001b[38;5;241m.\u001b[39mappend((shifts\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;241m+\u001b[39m base_anchors\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\modeling\\anchor_generator.py:43\u001b[0m, in \u001b[0;36m_create_grid_offsets\u001b[1;34m(size, stride, offset, target_device_tensor)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_grid_offsets\u001b[39m(\n\u001b[0;32m     40\u001b[0m     size: List[\u001b[38;5;28mint\u001b[39m], stride: \u001b[38;5;28mint\u001b[39m, offset: \u001b[38;5;28mfloat\u001b[39m, target_device_tensor: torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[0;32m     41\u001b[0m ):\n\u001b[0;32m     42\u001b[0m     grid_height, grid_width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m---> 43\u001b[0m     shifts_x \u001b[38;5;241m=\u001b[39m \u001b[43mmove_device_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_width\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_device_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     shifts_y \u001b[38;5;241m=\u001b[39m move_device_like(\n\u001b[0;32m     48\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(offset \u001b[38;5;241m*\u001b[39m stride, grid_height \u001b[38;5;241m*\u001b[39m stride, step\u001b[38;5;241m=\u001b[39mstride, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m     49\u001b[0m         target_device_tensor,\n\u001b[0;32m     50\u001b[0m     )\n\u001b[0;32m     52\u001b[0m     shift_y, shift_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(shifts_y, shifts_x)\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\jit\\_trace.py:1254\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing():\n\u001b[0;32m   1253\u001b[0m         \u001b[38;5;66;03m# Not tracing, don't do anything\u001b[39;00m\n\u001b[1;32m-> 1254\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1256\u001b[0m     compiled_fn: Callable[P, R] \u001b[38;5;241m=\u001b[39m script(wrapper\u001b[38;5;241m.\u001b[39m__original_fn)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dawoo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\detectron2\\layers\\wrappers.py:177\u001b[0m, in \u001b[0;36mmove_device_like\u001b[1;34m(src, dst)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_if_tracing\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove_device_like\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, dst: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    Tracing friendly way to cast tensor to another tensor's device. Device will be treated\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    as constant during tracing, scripting the casting process as whole can workaround this issue.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2 import model_zoo\n",
    "import numpy as np\n",
    "\n",
    "# Function to set up the custom Detectron2 model\n",
    "def get_custom_model(model_path, min_size):\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only one class (chair)\n",
    "    cfg.MODEL.WEIGHTS = model_path  # Path to the saved model\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # Increased confidence threshold for detection\n",
    "    cfg.INPUT.MIN_SIZE_TEST = min_size  # Set minimum size for the input\n",
    "\n",
    "    MetadataCatalog.get(\"chair_val\").thing_classes = [\"chair\"]  # Ensure metadata is registered\n",
    "\n",
    "    return DefaultPredictor(cfg)\n",
    "\n",
    "# Function to filter out small boxes\n",
    "def is_large_box(box, min_width=100, min_height=100):\n",
    "    x1, y1, x2, y2 = box\n",
    "    return (x2 - x1) > min_width and (y2 - y1) > min_height\n",
    "\n",
    "# Function to process the video\n",
    "def process_video_with_custom_model(input_path, model_path, min_size, output_path):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print('Error while trying to read video')\n",
    "        return\n",
    "\n",
    "    # Get frame width and height\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Define codec and create VideoWriter object\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    predictor = get_custom_model(model_path, min_size)\n",
    "\n",
    "    previous_boxes = []\n",
    "    previous_scores = []\n",
    "\n",
    "    # Read until end of video\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        outputs = predictor(frame)\n",
    "        instances = outputs[\"instances\"].to(\"cpu\")\n",
    "\n",
    "        boxes = instances.pred_boxes.tensor.numpy()\n",
    "        scores = instances.scores.numpy()\n",
    "        classes = instances.pred_classes.numpy()\n",
    "\n",
    "        current_boxes = []\n",
    "        current_scores = []\n",
    "\n",
    "        for box, score, class_id in zip(boxes, scores, classes):\n",
    "            if score > 0.7 and is_large_box(box):  # Confidence threshold and size filter\n",
    "                current_boxes.append(box)\n",
    "                current_scores.append(score)\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                label_name = f\"chair: {score:.2f}\"\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)  # Red bounding box\n",
    "                cv2.putText(frame, label_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Stabilize bounding boxes\n",
    "        if previous_boxes and previous_scores:\n",
    "            for prev_box, prev_score in zip(previous_boxes, previous_scores):\n",
    "                if any(np.allclose(prev_box, box, atol=10) for box in current_boxes):\n",
    "                    for i, (box, score) in enumerate(zip(current_boxes, current_scores)):\n",
    "                        if np.allclose(prev_box, box, atol=10):\n",
    "                            if abs(score - prev_score) < 0.1:  # If the score change is less than 10%\n",
    "                                current_boxes[i] = prev_box\n",
    "                                current_scores[i] = prev_score\n",
    "\n",
    "        previous_boxes = current_boxes\n",
    "        previous_scores = current_scores\n",
    "\n",
    "        print(f\"Frame {frame_count}: Detected {len(current_boxes)} instances\")\n",
    "\n",
    "        # Write the frame with detections to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Chair Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"Processed video saved as {output_path}\")\n",
    "\n",
    "# Parameters\n",
    "input_path = \"D:/ARTIFICIAL INTELLIGENCE/SEMESTER 1/Computer Vision and Deep Learning/CV_PROJECT_LAZY_TRAIN/human-pose-estimation-opencv/Chair_occupancy/Training_chair_detection_FasterRCNN/videoDet3.mp4\"\n",
    "model_path = \"D:/ARTIFICIAL INTELLIGENCE/SEMESTER 1/Computer Vision and Deep Learning/CV_PROJECT_LAZY_TRAIN/human-pose-estimation-opencv/Chair_occupancy/Training_chair_detection_FasterRCNN/output/model_final.pth\"\n",
    "min_size = 800\n",
    "output_path = \"D:/ARTIFICIAL INTELLIGENCE/SEMESTER 1/Computer Vision and Deep Learning/CV_PROJECT_LAZY_TRAIN/human-pose-estimation-opencv/Chair_occupancy/Training_chair_detection_FasterRCNN/video_output/output3_video_with_custom_model.mp4\"\n",
    "\n",
    "process_video_with_custom_model(input_path, model_path, min_size, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
